---
title: "Corp Table"
author: "Matthew"
date: "2024-04-19"
output: html_document
---

```{r}
library(haven)
data <- read_dta("/Users/homedepotsoup/Desktop/Econo2/IND_2009_EUS_V01_M_V06_A_GLD_ALL.dta")
```

```{r}
library(haven)
data2 <- read_dta("/Users/homedepotsoup/Desktop/Econo2/IND_2011_EUS_V01_M_V06_A_GLD_ALL.dta")
```

Load in the correspondence table; the code below creates a correspondence table between ISIC 4 and ISIC 3.1
```{r}
library(stringr)
setwd("~/Desktop/econo2")
correspondence <- read.csv("ISIC_words.txt", header = TRUE, stringsAsFactors = FALSE)
```

Drops the detail column, aggregates the data based on ISIC3.1 code
```{r}
correspondence <- correspondence[, !(names(correspondence) == "Detail")]

grouped_data <- split(correspondence$ISIC31code, correspondence$ISIC4code)

result_df <- data.frame(ISIC4code = character(), ISIC31code = character(), stringsAsFactors = FALSE)

for (code in names(grouped_data)) {
  isic31_codes <- unique(grouped_data[[code]])
  
  for (isic31_code in isic31_codes) {
    result_df <- rbind(result_df, data.frame(ISIC4code = code, ISIC31code = isic31_code))
  }
}

result_df$ISIC4code <- str_pad(result_df$ISIC4code, width = 4, side = "left", pad = "0")

result_df$ISIC31code <- str_pad(result_df$ISIC31code, width = 4, side = "left", pad = "0")

```

Creates the correspondence table
```{r}

get_correspondence_table <- function(correspondence_data) {
  agg_data <- aggregate(ISIC31code ~ ISIC4code, data = correspondence_data, FUN = unique)
  
  return(agg_data)
}

# For some reason R drops the 0 in front of codes like 0111, so this restores them
correspondence_table <- get_correspondence_table(result_df)
correspondence_table$ISIC4code <- sprintf("%s", correspondence_table$ISIC4code)


```

Creates a new dataframe with the 2011 ISIC code data to make prediction table; drops empty entries.
```{r}
ISIC4 <- data2$industrycat_isic_year

ISIC4[ISIC4 == ""] <- NA
ISIC4 <- na.omit(ISIC4)

df <- data.frame(ISIC4 =ISIC4, ISIC31 = NA)
# ISIC31 is empty so it can be populated in the next step
```

Matches Indian ISIC codes with the corresponding ones in 3.1.
```{r}

df$ISIC4 <- as.character(df$ISIC4)
correspondence_table$ISIC4code <- as.character(correspondence_table$ISIC4code)

for (i in seq_along(df$ISIC4)) {
  code <- df$ISIC4[i]
  match <- correspondence_table$ISIC4code == code
  if (any(match)) {
    df$ISIC31[i] <- correspondence_table$ISIC31code[match]
  }
}


```

Goes through each option and selects one for a new column, called selected_options, also takes probabilities and adds them to a new column as well. Prediction table!
```{r}
select_option_with_probability <- function(options) {
  if (is.vector(options)) {
    selected_option <- sample(options, 1)
    
    probability <- 1 / length(options)
    
    return(list(selected_option = selected_option, probability = probability))
  } else if (is.list(options)) {
    selected_option <- sample(options, 1)
    
    probability <- 1 / length(options)
    
    return(list(selected_option = selected_option, probability = probability))
  } else {
    return(NULL)
  }
}

selected_options <- lapply(df$ISIC31, select_option_with_probability)

df$selected_option <- sapply(selected_options, function(x) x$selected_option)
df$probability <- sapply(selected_options, function(x) x$probability)

print(df)
```
Start of constructing the correspondence table

```{r}
num_unique_isic4 <- length(unique(df$ISIC4))
print(paste("Number of unique ISIC4 codes:", num_unique_isic4))
```

This seperates both df into 2 df 1 is where prob is equal to 1 and df is otherwise
```{r}
# create a new dataframe with rows where 'probability' is 1
df1 <- df[df$probability == 1, ]

# remove the rows from the original dataframe where 'probability' is 1
df <- df[df$probability != 1, ]


print(head(df1))
print(head(df))

```
Create the weighted probabilities using the sum of job codes and proportions based on their mapping
```{r}
library(dplyr)
# new function to split the 'ISIC31' column and count the frequency of 'selected_option' to get our mapping
calculate_probabilities <- function(df) {
  # splitting the 'ISIC31' strings into separate rows
  split_df <- tidyr::separate_rows(df, ISIC31, sep = ",")
  
  # removing quotes and extra spaces from 'ISIC31' to clean up the data
  split_df$ISIC31 <- gsub('["c()]', '', split_df$ISIC31)
  split_df$ISIC31 <- trimws(split_df$ISIC31)
  
  # calculate the frequency of 'selected_option' corresponding to 'ISIC4'
  freq_df <- dplyr::count(split_df, ISIC4, ISIC31, selected_option)
  
  # calculate probabilities
  freq_df <- freq_df %>%
    group_by(ISIC4, ISIC31) %>%
    mutate(probability = n / sum(n))
  
  # get the correct columns for new df
  df2 <- freq_df %>% 
    select(ISIC4, ISIC31, probability) %>%
    distinct()
  
  return(df2)
}

df2 <- calculate_probabilities(df)


print(head(df2))

```

Cleaning up the df to better represent the selected option in the mapping
```{r}
library(dplyr)
library(tidyr)

# clean the 'ISIC31' column by removing the 'c()', quotes, and splitting it into separate rows for each code
df <- df %>% 
  mutate(ISIC31 = gsub("c\\(|\\)|\"", "", ISIC31)) %>%
  separate_rows(ISIC31, sep = ",\\s*")

# get the frequency for ecah selected option in 4.0
df2 <- df %>%
  group_by(ISIC4, ISIC31, selected_option) %>%
  summarise(count = n(), .groups = "drop") %>%
  mutate(probability = count / sum(count)) %>%
  ungroup() %>%
  filter(ISIC31 == selected_option)  # making sure ISIC31 matches the selected_option

print(head(df2))

```
The final product for now is df3 as the correspondence table
```{r}
df3 <- df2 %>%
  group_by(ISIC4) %>%
  mutate(probability = count / sum(count)) %>%
  ungroup() %>%
  select(ISIC4, ISIC31, selected_option, probability) 

# Remove the column 'selected_option' from df3
df3 <- df3[ , !(names(df3) %in% c("selected_option"))]

# View the updated dataframe
print(df3)

print(head(df3))
print(df3)
```

Now we read in the tools results 
```{r}
library(readxl)

# Specify the path to your Excel file
file_path <- "~/Desktop/correspondence_table.xlsx"  
df6 <- read_excel(file_path)
# Remove the first column (ID column)
df6 <- df6[, -1]

# Rename the remaining columns to match the desired column names
names(df6) <- c("ISIC4", "ISIC31", "probability")

# View the updated dataframe
print(df6)

```
Use the tables results to match the missing probailties of 1's in the india data set so now both the results and the india occupation data probailties have even columns
```{r}
# Find rows in df6 where probability is 1
df6_prob_1 <- df6[df6$probability == 1, ]

# Calculate the number of rows to add to df3
rows_to_add <- nrow(df6) - nrow(df3)

# Select the required number of rows from df6 where probability is 1
rows_to_add <- df6_prob_1[1:rows_to_add, ]

# Bind the rows to df3
df3 <- rbind(df3, rows_to_add)

# View the updated df3
print(df3)

```
df 7 which is the India data, now has the same amount of columns as the tools results 
```{r}
# Assuming df3 and df6 have the same columns and data types

# Create a key for matching
df6$key <- paste(df6$ISIC4, df6$ISIC31, sep = "_")
df3$key <- paste(df3$ISIC4, df3$ISIC31, sep = "_")

# Match the order of df3 to df6 using the key
df3_ordered <- df3[match(df6$key, df3$key), ]

# Remove the key column from the reordered df3
df3_ordered$key <- NULL

# Create df7 from the ordered df3
df7 <- df3_ordered

# View the new dataframe df7
print(df7)
```

```{r}
# Count the number of NA values in df7
na_count <- sum(is.na(df7))

# Print the number of NA values
print(paste("Number of NA values in df7:", na_count))
```
Noticed that there are a ton of NA Values(possibly due to some job codes not having any observations in India)
```{r}
# Remove the key column from df6
df6$key <- NULL
```
THE DIFFERENCES BETWEEN OUR TOOL RESULTS AND THE INDIA OBSERVED OCCUPATION PROBAILITY
```{r}
# Ensure both dataframes have the same order and aligned rows
# Calculate the difference between the probabilities
prob_diff <- df6$probability - df7$probability

# Round the prob_diff column to the nearest hundredth
prob_diff <- round(prob_diff, 2)

# Create the new dataframe df_diff with the required columns
df_diff <- data.frame(
  ISIC4 = df6$ISIC4,
  ISIC31 = df6$ISIC31,
  prob_diff = prob_diff
)

# Set scipen option to remove scientific notation
options(scipen = 999)

# View the new dataframe df_diff
print(df_diff)
```
The probaility differences between the results and the obersved worked, but 141 missing NA values is pretty high, so going to retry without the proabilites of 1. df10 is just df3 again with no probailites of 1 from the India data.
```{r}
df10 <- df2 %>%
  group_by(ISIC4) %>%
  mutate(probability = count / sum(count)) %>%
  ungroup() %>%
  select(ISIC4, ISIC31, selected_option, probability) 

# Remove the column 'selected_option' from df3
df10 <- df10[ , !(names(df10) %in% c("selected_option"))]

# View the updated dataframe
print(df10)
```
df11 is the tools result without the probalities of 1
```{r}
# Drop all rows where probability is 1
df11 <- subset(df6, probability != 1)
print(df11)
```
df12 contains the final proability differences between the results and India occupation codes with only 47 missing values 
```{r}
library(dplyr)

# Create a key for matching
df11$key <- paste(df11$ISIC4, df11$ISIC31, sep = "_")
df10$key <- paste(df10$ISIC4, df10$ISIC31, sep = "_")

# Merge df11 and df10 to align the rows
merged_df <- merge(df11, df10, by = "key", suffixes = c("_df11", "_df10"), all.x = TRUE)

# Calculate the difference between the probabilities
merged_df$prob_diff <- merged_df$probability_df11 - merged_df$probability_df10

# Create the new dataframe df12 with the required columns and filter out missing values
df12 <- merged_df %>%
  filter(!is.na(probability_df10)) %>%
  select(ISIC4 = ISIC4_df11, ISIC31 = ISIC31_df11, prob_diff)

# Set scipen option to remove scientific notation and round to two decimal places
options(scipen = 999)
df12$prob_diff <- round(df12$prob_diff, 2)

# Remove the key columns
df11$key <- NULL
df10$key <- NULL

print("df12:")
print(df12)
```

Here are the missing values from the probaility differences 
```{r}
# Separate rows where df10 has missing values
df_missing <- merged_df %>% filter(is.na(probability_df10))

print("df_missing:")
print(df_missing)
```

